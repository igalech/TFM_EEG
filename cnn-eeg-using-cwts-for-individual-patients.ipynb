{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom random import sample\nfrom datetime import datetime\n\n!pip install -q tensorflow_addons\nimport tensorflow_addons as tfa\n\n!pip install mne\nimport mne\n\n!pip install wfdb pyEDFlib PyWavelets eeglib wfdb pyeeg\n\n%matplotlib inline\nimport wfdb\nimport pyedflib\nimport pywt \nimport eeglib\n\nfrom mne import Epochs, create_info, events_from_annotations\nfrom mne.io import concatenate_raws, read_raw_edf\nfrom mne.decoding import CSP\nfrom mne.time_frequency import AverageTFR\nfrom mne.decoding import Vectorizer\nfrom mne.io import concatenate_raws, read_raw_edf\nfrom mne import Epochs, create_info, events_from_annotations, pick_events, pick_types\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-08T17:02:02.965462Z","iopub.execute_input":"2021-12-08T17:02:02.966011Z","iopub.status.idle":"2021-12-08T17:02:52.666628Z","shell.execute_reply.started":"2021-12-08T17:02:02.965897Z","shell.execute_reply":"2021-12-08T17:02:52.665354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cargamos la lista de todos los seizures y sus tiempos en cada fichero\ndf_seizures = pd.read_csv('/kaggle/input/eeg-edfs-files-curated/seizures summary.csv',delimiter=';', encoding='utf8')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:03:45.745203Z","iopub.execute_input":"2021-12-08T17:03:45.745495Z","iopub.status.idle":"2021-12-08T17:03:45.762524Z","shell.execute_reply.started":"2021-12-08T17:03:45.745462Z","shell.execute_reply":"2021-12-08T17:03:45.761714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mne.io import concatenate_raws, read_raw_edf\nfrom mne import Epochs, create_info, events_from_annotations, pick_events, pick_types\n\nlista_erroneos = ['chb10_31.edf', 'chb04_05.edf','chb04_08.edf', 'chb09_19.edf', 'chb09_06.edf', 'chb06_18.edf','chb10_20.edf',\n                  'chb10_38.edf', 'chb10_12.edf', 'chb23_06.edf', 'chb06_10.edf', 'chb06_24.edf', 'chb06_09.edf', 'chb07_12.edf', \n                  'chb10_27.edf']\n\ndf_seizures = pd.read_csv('/kaggle/input/eeg-edfs-files-curated/seizures summary.csv',delimiter=';', encoding='utf8')\n\ndf_seizures = df_seizures[~df_seizures['nombre archivo'].isin(lista_erroneos)]\n\ndef lee_raw(paciente):\n    \n    #elegidos=[]\n\n    #for i in np.random.choice(df_seizures['paciente'].unique(), 1):\n    #for i in paciente:\n        #np.random.choice(df_seizures['paciente'].unique(), 1):\n        #elegidos.append(np.random.choice(df_seizures[df_seizures['paciente'] == i]['nombre archivo']))\n    #elegidos.append(np.random.choice(df_seizures[df_seizures['paciente'] == paciente]['nombre archivo'],1) )\n    \n    #elegidos.append(df_seizures[df_seizures['paciente'] == paciente]['nombre archivo'].sample().to_list()[0])\n    \n    elegido = df_seizures[df_seizures['paciente'] == paciente]['nombre archivo'].sample().to_list()[0]\n\n    # Path with the recordings\n    myPath = '/kaggle/input/eeg-edfs-files-curated/edfs/'\n    # Ficheros totales (All patients)\n    data = os.listdir(myPath)\n    \n    #epochs_all llevará todos los epochs de todos los pacientes que seleccionemos.\n    epochs_all = np.empty(shape=())  # Empty array with size =  1\n    data_all = np.empty(shape=()) # Empty array with size =  1\n    epochs_all_10 = np.empty(shape=())  # Empty array with size =  1\n\n    count = 0\n    \n    #print(\"Elegidos : {}\".format(elegidos))\n    #for i in elegidos:\n        #print(\"type de i:{} and {}\".format(type(i), i))\n        #dataPath = os.path.join(myPath, data[i])\n    \n    dataPath = os.path.join(myPath, str(elegido))\n        \n    # We read our EDF recording and build a generic 'mapping' of channels that\n    # we will remove since they are wrong or reference channels.\n    mapping = {'EOG horizontal': 'eog', 'Resp oro-nasal': 'misc',\n               'EMG submental': 'misc', 'Temp rectal': 'misc', \n               'Event marker': 'misc', '-': 'misc', '.': 'misc', 'T8-P8':'T8-P8',\n               'FC1-Ref':'FC1-Ref', 'FC2-Ref':'FC2-Ref', 'FC5-Ref':'FC5-Ref',\n               'FC6-Ref':'FC6-Ref', 'CP1-Ref':'CP1-Ref','CP2-Ref':'CP2-Ref',\n               'CP5-Ref':'CP5-Ref', 'CP6-Ref':'CP6-Ref', 'PZ-OZ':'PZ-OZ'}\n    \n        # We will specifically exclude the channel with issues (T8-P8) and the weird channels (. and -)\n    excluded_channels = mapping.keys()\n        \n    if str(elegido) in df_seizures['nombre archivo'].to_list():\n        \n        print(\"Leyendo fichero...:{}\".format(elegido))\n        print(\"Desde :{}\".format(myPath+elegido))\n        count +=1\n        raw = mne.io.read_raw_edf(myPath+elegido, exclude=excluded_channels, preload=True, stim_channel='auto', verbose=None)\n     \n        raw = raw.resample(sfreq=127)\n\n        #raw = mne.io.read_raw_edf(path, exclude=['-', 'T8-P8', '.'], verbose=False, preload=True)\n\n        # Rename EEG channels\n        ch_names = {i: i.replace('EEG ', '') for i in raw.ch_names if 'EEG' in i}\n        mne.rename_channels(raw.info, ch_names)\n        \n        \n        \n        # We then apply notch filter around 60 Hz. (USA) or 50Hz (Europe) This is recommended by some papers to eliminate noise from electrodes (power line).\n        raw = raw.notch_filter(freqs=[59.1, 60.9])\n\n        # We then apply the alpha and beta filters that go from 7 to 26/28 Hz (FIRWIN type, default for MNE/Scipy)\n        raw = raw.filter(l_freq=8.0, h_freq=29.0)\n\n        # Now we mark the starts and ends of the seizures to obtain the class (Inter, Preictal, Ictal) for each second.\n        seizure_init = float(df_seizures[df_seizures['nombre archivo']==elegido]['inicio seizure segundos'].to_list()[0])\n        # print(\"seiz init\", seizure_init)\n        seizure_end = float(df_seizures[df_seizures['nombre archivo']==elegido]['fin seizure segundos'].to_list()[0])\n        duration_seiz = float(seizure_end-seizure_init)\n    \n        preictal_init = float(seizure_init - 10*60)  # 10 minutos (se puede aumentar)\n        preictal_duration = float(seizure_init - preictal_init)\n        print(\"seiz_init, preictal_duration y seizure_end\", seizure_init, preictal_duration, seizure_end)\n\n        normal_antes_pre_dur = float(preictal_init)\n        normal_despues_seiz_dur = float(3600 - seizure_end)\n\n        this_file_annots = mne.Annotations(onset=[0, preictal_init, seizure_init, seizure_end ],  # in seconds\n            duration=[normal_antes_pre_dur, preictal_duration, duration_seiz, normal_despues_seiz_dur],  # in seconds, too\n            description=['Inter','Preictal','Ictal', 'Inter'],\n            ch_names = None)\n        \n        raw.set_annotations(this_file_annots)\n        \n                \n        # Recogemos los eventos de las anotaciones (para construir los futuros epochs a partir de eventos)\n        # También podríamos usar duration=2.5 y overlap=0.5\n        events, events_ids = mne.events_from_annotations(raw, event_id={'Preictal':0, 'Ictal':2, 'Inter':1}, \n                                                         chunk_duration=1, use_rounding=True )\n        \n        # Finally we crop the first minute to reduce noise (1 m. until patient settles)\n        crop_file_mins = 1\n        if crop_file_mins > 0:  # Cut start of file\n            # Crop raw\n            tmin_crop = crop_file_mins * 60\n            #tmax = crop_file_mins * 60\n            raw.crop(tmin=tmin_crop)\n        \n        # Save subject and recording information in raw.info\n        # Guardamos la info del fichero: paciente/fichero.\n        basename = elegido\n        #print(int(basename[3:5]))\n\n        subj_nb, rec_nb = str(basename[3:6]), str(basename[7:8])\n        # print(\"Los integers\", subj_nb, rec_nb)\n\n        raw.info['patient_info'] = {'id': subj_nb, 'fichero_id': rec_nb}\n\n        print('Patient_info info: {}'.format(raw.info['patient_info'])+'\\n')\n\n        if count == 1:\n\n            raw_final = raw.get_data()\n\n        else:\n\n            raw_final = np.append(raw_final, raw.get_data(), axis=1)\n\n        #mne.concatenate_epochs(epochs_list, add_offset=True, *, on_mismatch='raise', verbose=None)\n\n        picks = pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n\n        # Jugando con tmin y tmax podemos obtener epochs de mas o menos duracion y/o solapamiento \n        epochs_all_tmp = mne.Epochs(raw, picks=picks, events=events, event_repeated = 'merge', \n                         event_id=events_ids, tmin=-0.0001, tmax=1.0, baseline =(None,None),\n                         preload=True)\n        \n        # Construimos epochs (con el objeto Raw epochs_all_tmp de cada fichero).\n        if data_all.size > 1:\n\n            #epochs_all = np.append(epochs_all, epochs_all_tmp.get_data(), axis=0)\n\n            epochs_PIN = epochs_all_tmp['Preictal', 'Ictal', 'Inter']\n            final_df = final_df.append(epochs_PIN.to_data_frame())\n\n            data_all_tmp = epochs_PIN.get_data()\n            print('size of tmp : {}'.format(data_all_tmp.shape) +'\\n')\n            data_all = np.append(data_all, data_all_tmp, axis=0)\n\n            #labels_data_tmp = epochs_all_tmp.events[:,-1]\n\n            labels_data_all = np.append(labels_data_all, epochs_PIN.events[:,-1], axis = 0)\n\n            #epochs_all_10 = np.append(epochs_all_10, epochs_all_tmp_10, axis=0)\n\n        else:\n            \n            epochs_all = mne.Epochs(raw, picks=picks, events=events, event_repeated = 'merge', \n                         event_id=events_ids, tmin=-0.0001, tmax=1.0, baseline =(None,None),\n                         preload=True)\n\n            epochs_PIN = epochs_all['Preictal', 'Ictal', 'Inter']\n                \n            # Para 'unificar' clases por tamanos. \n            # epochs_PIN.equalize_event_counts(list(events_ids.keys()))\n                \n            final_df = epochs_PIN.to_data_frame()\n\n            data_all = epochs_PIN.get_data()\n\n            #print('size of dataall :{}'.format(data_all.shape) + '\\n')\n            labels_data_all = epochs_PIN.events[:,-1]\n\n            #epochs_all_10 = epochs_all_tmp_10\n\n#    else:\n\n#        continue\n    final_df['cat'] = [1 if i == 'Preictal' else 2 if i == 'Ictal' else 0 for i in final_df['condition']]\n    \n    return final_df","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:03:53.44081Z","iopub.execute_input":"2021-12-08T17:03:53.441083Z","iopub.status.idle":"2021-12-08T17:03:53.486158Z","shell.execute_reply.started":"2021-12-08T17:03:53.441047Z","shell.execute_reply":"2021-12-08T17:03:53.485345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_cwt(DF, wavelet, ventana=10, solapamiento=0, fs = 128):\n    \n    fs = fs\n    final_df = DF\n    waveletname = pywt.ContinuousWavelet(wavelet)\n    escalas = pywt.scale2frequency(waveletname.name, np.arange(8,30.5,2)) / (1/fs)\n    print(\"Numero de escalas que vamos a probar : {}\".format(len(escalas)))\n    ventana = ventana\n    incremento = fs * ventana  # La ventana me da el incremento o 'avance' en cada iteracion\n    solapamiento = int(solapamiento * fs)\n    \n    train_size = int(np.floor(final_df.shape[0]/(ventana*fs)))\n    \n    n_channels = len(final_df.columns) - 4\n    # Redondeamos para evitar el último epoch que desbordaría\n    tamano_resultado_train = int(round(train_size,0))\n    \n    # Matriz resultado en train (tamaño muestras, escalas, freq.samp, canales)\n    train_data_cwt = np.ndarray(shape=(tamano_resultado_train, len(escalas), fs, n_channels))\n    train_data_cwt_labels = np.ndarray(shape=(tamano_resultado_train, 1, n_channels))\n    \n    for ii in range(0,tamano_resultado_train):\n        if ii == int(np.floor(tamano_resultado_train/2)):\n            print(\"CWT en set training a la mitad...{}\".format(ii*incremento))\n    \n        for jj in range(0,n_channels):\n        \n            start = final_df['epoch'].min()\n \n            signal = final_df.iloc[ii*(incremento - solapamiento):(ii+1)*(incremento-solapamiento), jj+3:jj+4]\n    \n            # Calculamos la clase con el 'techo' del valor de la mediana de cada ventana\n            clase_intervalo = int(np.ceil(np.median(final_df.iloc[ii*(incremento- solapamiento) : (ii+1)*(incremento- solapamiento), -1])))\n  \n            train_data_cwt_labels[ii, : ,jj] = clase_intervalo\n        \n            coeff, freq = pywt.cwt(signal, escalas, waveletname, 1)\n        \n            coeff_approx = coeff[:,:fs]\n        \n            train_data_cwt[ii, :, :, jj] = abs(coeff_approx[:,:,-1])\n    \n    print(\"Final CWT decomp. \\n\") \n    \n    return train_data_cwt, train_data_cwt_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:04:08.120196Z","iopub.execute_input":"2021-12-08T17:04:08.120905Z","iopub.status.idle":"2021-12-08T17:04:08.135372Z","shell.execute_reply.started":"2021-12-08T17:04:08.120853Z","shell.execute_reply":"2021-12-08T17:04:08.134574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fs = 128\nescalas = pywt.scale2frequency('cmor3-3', np.arange(8,30.5,2) / (1/fs))\nprint(\"Numero de escalas que vamos a probar : {}\".format(len(escalas)))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:03:36.472114Z","iopub.execute_input":"2021-12-08T17:03:36.47248Z","iopub.status.idle":"2021-12-08T17:03:36.478349Z","shell.execute_reply.started":"2021-12-08T17:03:36.472442Z","shell.execute_reply":"2021-12-08T17:03:36.477809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resultados individualizados\n\nsegundos = [30]\nwavelets = ['mexh']\nsolapamientos = [0, 1]\nresultados = dict()\n#for i in df_seizures['paciente'].unique():\nfor i in np.random.choice(df_seizures['paciente'].unique(),1).tolist():\n    temp = lee_raw(i)\n    #lectura_edf = obtain_edf(i)\n    for j in segundos:\n        for k in wavelets:        \n            #resultados.update({'paciente_'+i+'_'+str(j)+'_'+k: temp})\n            for l in solapamientos:\n                resultados.update({'paciente_'+i+'_'+str(j)+'_'+k+'solap_'+str(l): compute_cwt(temp, k, j, l)})","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:04:31.705696Z","iopub.execute_input":"2021-12-08T17:04:31.706626Z","iopub.status.idle":"2021-12-08T17:26:01.473192Z","shell.execute_reply.started":"2021-12-08T17:04:31.706577Z","shell.execute_reply":"2021-12-08T17:26:01.4719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D\nfrom keras.models import Sequential\nfrom keras.callbacks import History \n#history = History()\n\nnum_escalas = list(resultados.values())[0][0].shape[1]\ncoeficientes = list(resultados.values())[0][0].shape[2]\ncanales = list(resultados.values())[0][0].shape[3]\ninput_shape = (num_escalas, coeficientes , canales)\n\nnum_classes = 3\nbatch_size = 16\nepochs = 50\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\nmodel.add(Conv2D(64, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                metrics=['accuracy',\n                tfa.metrics.F1Score(name= 'f1_score', num_classes=3),\n                tf.keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),\n                tf.keras.metrics.Precision(name='precision'),\n                tf.keras.metrics.Recall(name='recall')])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:26:01.476018Z","iopub.execute_input":"2021-12-08T17:26:01.47642Z","iopub.status.idle":"2021-12-08T17:26:01.722925Z","shell.execute_reply.started":"2021-12-08T17:26:01.476349Z","shell.execute_reply":"2021-12-08T17:26:01.722092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    METRICS = ['accuracy',\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'),\n      tfa.metrics.F1Score(name= 'f1_score', num_classes=3),\n      tf.keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:34:07.506626Z","iopub.execute_input":"2021-12-08T17:34:07.507351Z","iopub.status.idle":"2021-12-08T17:34:07.542075Z","shell.execute_reply.started":"2021-12-08T17:34:07.507306Z","shell.execute_reply":"2021-12-08T17:34:07.541472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_callbacks(model, metric = METRICS):\n    \n    #guardar_path = '/home/inaki/temp/mejor_modelo_'+ model.name + '.h5'\n    \n    #checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    #    filepath = guardar_path,\n    #    monitor = metric,\n    #    mode ='max',\n    #    save_best_only=True,\n    #    verbose=1,\n    #)\n\n    reduccionlr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = metric,\n        mode = 'max',\n        factor = 0.1,\n        patience = 3,\n        verbose = 0\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor = metric,\n        mode ='max',\n        patience = 5, \n        verbose = 1\n    )\n    \n    callbacks = [reduccionlr, earlystop]         \n    \n    return callbacks","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:12:42.053645Z","iopub.execute_input":"2021-11-12T21:12:42.053958Z","iopub.status.idle":"2021-11-12T21:12:42.059805Z","shell.execute_reply.started":"2021-11-12T21:12:42.053918Z","shell.execute_reply":"2021-11-12T21:12:42.059186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_model(data, labels):\n    from random import sample\n    \n    train_data_cwt = data\n    train_data_cwt_labels = labels\n    \n    #print(train_data_cwt.shape)\n    #print(train_data_cwt_labels.shape)\n    \n    items = [item for sublist in train_data_cwt_labels[:,:,-1] for item in sublist]\n\n    import collections\n\n    # using Counter from Dict() to find frequency of elements\n    balanceo = collections.Counter(items)\n    suma = sum(balanceo.values())\n    for k, v in balanceo.items():\n        balanceo[k] = (1/v)*(suma/3)\n    print(\"Balanceo clases : {}\".format(dict(balanceo)))\n\n    from sklearn.preprocessing import StandardScaler\n\n    scaler = StandardScaler()\n    #num_instances, escalas, coef, num_canales = train_data_cwt.shape\n    train_data = np.reshape(train_data_cwt, (-1, train_data_cwt.shape[-1]))\n    train_data = scaler.fit_transform(train_data)\n    train_data = np.reshape(train_data, (train_data_cwt.shape[0], train_data_cwt.shape[1], train_data_cwt.shape[2], train_data_cwt.shape[-1]))\n    \n    random_num_generated = int(sample(range(0,99), 1)[0])\n    \n    # Ensure that we get at least one sample of each class on both train and test\n    X_train, X_test, y_train, y_test = train_test_split(train_data, train_data_cwt_labels,  test_size=0.3, random_state = random_num_generated)\n    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n    \n    y_train_cat = tf.keras.utils.to_categorical(y_train[:,-1,-1], 3)\n    y_test_cat = tf.keras.utils.to_categorical(y_test[:,-1,-1], 3)\n    \n    #print(\"Shapes\", y_train_cat.shape, y_test_cat.shape)\n    \n    # Ensure that we get at least one sample of each class on both train and test\n    while len(set(np.argmax(y_test_cat, axis=1).tolist())) < len(set(np.argmax(y_train_cat, axis=1).tolist())):\n        random_num_generated = int(sample(range(0,99), 1)[0])\n        X_train, X_test, y_train, y_test = train_test_split(train_data, train_data_cwt_labels,  test_size=0.3, random_state = random_num_generated)\n        y_train_cat = tf.keras.utils.to_categorical(y_train[:,-1,-1], 3)\n        y_test_cat = tf.keras.utils.to_categorical(y_test[:,-1,-1], 3)\n    \n    #print(\"Shapes\", y_train_cat.shape, y_test_cat.shape)\n    \n    batch_size = 16\n    epochs = 50\n    \n    METRICS = ['accuracy',\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'),\n      tfa.metrics.F1Score(name= 'f1_score', num_classes=3),\n      tf.keras.metrics.CategoricalCrossentropy(name='categorical_crossentropy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n              metrics= METRICS)\n    \n    history = History()\n    history = model.fit(X_train, y_train_cat,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(X_test, y_test_cat),\n          #callbacks = create_callbacks(model),\n          callbacks = [history],\n          class_weight = balanceo)\n\n    ultimos = dict()\n    \n    for k, v in history.history.items():\n        ultimos.update({k: v[-1]})  \n    \n    return ultimos","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:12:42.061236Z","iopub.execute_input":"2021-11-12T21:12:42.061448Z","iopub.status.idle":"2021-11-12T21:12:42.079852Z","shell.execute_reply.started":"2021-11-12T21:12:42.06142Z","shell.execute_reply":"2021-11-12T21:12:42.078929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Operamos ahora con los resultados de todos los CWTs obtenidos por paciente, ejecutando el modelo CNN para cada uno de los pacientes y guardando los resultados.","metadata":{}},{"cell_type":"code","source":"historias = dict()\nfor k, v in resultados.items():\n    #a = v[0]\n    #b = v[1]\n    #print(a.shape, b.shape)\n    hist_temp = process_model(v[0], v[1])\n    historias.update({k:hist_temp})","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:12:42.081277Z","iopub.execute_input":"2021-11-12T21:12:42.081644Z","iopub.status.idle":"2021-11-12T21:29:16.272985Z","shell.execute_reply.started":"2021-11-12T21:12:42.081592Z","shell.execute_reply":"2021-11-12T21:29:16.272277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#historias_df.head()\n#for k, v in resultados.items():\n#    print(k, v[0].shape[1], v[0].shape[2], v[0].shape[3])\n#        #v[0][0].shape[1],v[0][0].shape[2],v[0][0].shape[3])","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:32:33.569402Z","iopub.execute_input":"2021-11-12T21:32:33.570092Z","iopub.status.idle":"2021-11-12T21:32:33.594245Z","shell.execute_reply.started":"2021-11-12T21:32:33.570048Z","shell.execute_reply":"2021-11-12T21:32:33.593424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nhistorias_df = pd.DataFrame(historias)\nnow = datetime.now()\ndt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\nprint(dt_string)\nhistorias_df.to_csv('/kaggle/working/historias_df'+dt_string+'.csv', sep=',', na_rep='', header=True, index=True, mode='w')","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:45:40.166338Z","iopub.execute_input":"2021-11-12T21:45:40.167297Z","iopub.status.idle":"2021-11-12T21:45:40.23644Z","shell.execute_reply.started":"2021-11-12T21:45:40.167224Z","shell.execute_reply":"2021-11-12T21:45:40.235841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nimport base64\n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = f'<a target=\"_blank\">{title}</a>'\n    return HTML(html)\n\n\n#historias_df.to_csv('/kaggle/working/historias_df'+dt_string+'.csv', sep=',', na_rep='', header=True, index=True, mode='w')","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:43:16.083523Z","iopub.execute_input":"2021-11-12T21:43:16.083802Z","iopub.status.idle":"2021-11-12T21:43:16.089406Z","shell.execute_reply.started":"2021-11-12T21:43:16.083772Z","shell.execute_reply":"2021-11-12T21:43:16.088586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create_download_link(historias_df, filename = '/kaggle/working/historias_df'+dt_string+'.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:44:25.097748Z","iopub.execute_input":"2021-11-12T21:44:25.098457Z","iopub.status.idle":"2021-11-12T21:44:25.114569Z","shell.execute_reply.started":"2021-11-12T21:44:25.09841Z","shell.execute_reply":"2021-11-12T21:44:25.113725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode = pywt.Modes.smooth\n\ndef dwt_signal_decomp(data, w):\n    \"\"\"Decompose and plot a signal S.\n    S = An + Dn + Dn-1 + ... + D1\n    \"\"\"\n    w = pywt.Wavelet(w)\n    a = data\n    ca = []\n    cd = []\n    for i in range(5):\n        (a, d) = pywt.dwt(a, w, mode)\n        ca.append(a)\n        cd.append(d)\n\n    rec_a = []\n    rec_d = []\n\n    for i, coeff in enumerate(ca):\n        coeff_list = [coeff, None] + [None] * i\n        rec_a.append(pywt.waverec(coeff_list, w))\n\n    for i, coeff in enumerate(cd):\n        coeff_list = [None, coeff] + [None] * i\n        rec_d.append(pywt.waverec(coeff_list, w))\n        \n    return ca, cd","metadata":{"execution":{"iopub.status.busy":"2021-11-12T21:29:16.308383Z","iopub.execute_input":"2021-11-12T21:29:16.308598Z","iopub.status.idle":"2021-11-12T21:29:16.315489Z","shell.execute_reply.started":"2021-11-12T21:29:16.308572Z","shell.execute_reply":"2021-11-12T21:29:16.314718Z"},"trusted":true},"execution_count":null,"outputs":[]}]}